{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ca6c50b-45d8-4292-937f-df72ebf5dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.14.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/codespace/.local/lib/python3.12/site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5fc20ec-44d1-4eca-ae3c-4d7aa2601778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6aab9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"/workspaces/scrape/.gitignore/.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f662fed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IXcCcUM85jLp6G2g40nMiy6a3\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Test if environment variable is being loaded\n",
    "print(os.getenv(\"API_KEY\"))  # Should print your API key if loaded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09f63eec-0626-424f-9c1a-bd0cc382de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# Replace with your credentials\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_secret = os.getenv(\"API_SECRET\")\n",
    "bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "\n",
    "# Authenticate with the Bearer Token\n",
    "try:\n",
    "    client = tweepy.Client(bearer_token=bearer_token)\n",
    "    print(\"Authentication successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during authentication: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22e41387-7491-4076-be31-37b24b827f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Tweets:\n",
      "6526 (æ ª)ã‚½ã‚·ã‚ªãƒã‚¯ã‚¹ãƒˆ \n",
      "-123.5(-4.65%)\n",
      "è‡ªç¤¾æ ªè²·ã„ã®å®Ÿæ–½ã‚’ç™ºè¡¨ã€\n",
      "ã³ã£ãã‚Šã—ãŸã‚ ã“ã‚Œæ˜Žæ—¥çˆ†ä¸Šã’ã˜ã‚ƒã‚“\n",
      "https://t.co/uJ8g0EhNku\n",
      "@itsCblast Stocks\n",
      "IT stocks cracked under pressure on December 31, with industry stalwarts like TCS, Infosys, and Tech Mahindra dropping 2-3%.\n",
      "https://t.co/ZQ0NhE0cQ1\n",
      "@aixbt_agent @otaku_r3b Yo @aixbt_agent, scaling is key in the game, but don't sleep on innovation! ðŸš€ In the world of Web3 and GameFi, it's not just about the moat you've built, but how you navigate the galaxy. $AUDIT might be onto something fresh, blending stocks with crypto vibes.\n",
      "This partnership blends traditional finance with blockchain technology, offering investors exposure to real-world assets (RWAs) like US Treasuries and stocks.\n",
      "7267 ãƒ›ãƒ³ãƒ€ \n",
      "+4(0.26%)\n",
      "ä»ŠæœŸé…å½“äºˆæƒ³å¢—é¡ã¨ï¼‘å¯¾ï¼’ã®æ ªå¼åˆ†å‰²ã®ç™ºè¡¨ã‚’ææ–™è¦–\n",
      "æ˜Žæ—¥ã¯Sé«˜è¡Œãã®ã‹ã„ï¼Ÿ\n",
      "https://t.co/LxCgU6NfBH\n",
      "Top 10 stocks have accounted for 59% of the S&amp;P 500â€™s gains since the October 2022 bottom. https://t.co/Nacsp7rkOD\n",
      "RT @fomohour: BTC hits 2-week High, China Stocks Rally again, GOAT hits $120M https://t.co/YZHZjoFjzK\n",
      "Those who are still buying non-fundamental stocks. ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ https://t.co/ClKrERv6jW\n",
      "6723 ãƒ«ãƒã‚µã‚¹ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ‹ã‚¯ã‚¹(æ ª)\n",
      "-18.5(-0.9%) \n",
      "å‰æ—¥ã‚‚ã‚¹ãƒˆãƒƒãƒ—é«˜ã€‚äººå·¥çŸ¥èƒ½é–¢é€£\n",
      "æµçŸ³ã§ã™ã€‚\n",
      "https://t.co/IDbSkuBWA4\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Search for recent tweets containing the term \"stocks\"\n",
    "    response = client.search_recent_tweets(query=\"stocks\", max_results=10)\n",
    "    \n",
    "    # Display the tweets\n",
    "    if response.data:\n",
    "        print(\"Recent Tweets:\")\n",
    "        for tweet in response.data:\n",
    "            print(tweet.text)\n",
    "    else:\n",
    "        print(\"No tweets found for the given query.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching tweets: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4521b28-4efc-4200-814d-84f9f9359fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ed6f15-3c71-47aa-b357-9f8e3b6f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #for regular expression to clean text\n",
    "import nltk #Natural language Toolkit for text processing\n",
    "from nltk.corpus import stopwords #To remove common, meaningless words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #To break text into tokens (individual words)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc73c9f-bc68-47c8-a00b-4214a102c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #contains words like \"and\",\"the\",\"is\" etc., which are filtered -> don't carry any significant meaning.\n",
    "nltk.download('punkt_tab') #a pre-trained tokenizer model for splitting text into sentences and words\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a9a15d-8abe-48e5-9c53-4f4777c19a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#What the package stopwords contains\n",
    "print(stopwords.words('english'))  # Prints English stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052cc4af-905e-42a4-b5a6-704cb31555b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
      "['Hello, world!', 'NLP is amazing.']\n"
     ]
    }
   ],
   "source": [
    "#how punkt Tokenizer works\n",
    "text = \"Hello, world! NLP is amazing.\"\n",
    "print(word_tokenize(text))  # Outputs: ['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
    "print(sent_tokenize(text))  # Outputs: ['Hello, world!', 'NLP is amazing.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f62ae2-0e0b-4ea6-b2f9-ffe1e0c3c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "#how WordNetLemmatizer works\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, pos=\"v\")  # Specify part of speech (v = verb)\n",
    "print(lemma)  # Output: \"run\"\n",
    "print(lemmatizer.lemmatize(\"better\"))  # Output: \"better\" (default POS is noun)\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Output: \"good\" (adjective POS)\n",
    "#pos=\"n\"-Noun \"v\"=Verb \"a\"=Adjective \"r\"-Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61097b13-5420-4096-9afd-cf20bb2148a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing stopwords and lemmatizer\n",
    "stop_words= set(stopwords.words('english'))\n",
    "lemmantizer=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #Step 1: remove URLs\n",
    "    tweet =re.sub(r\"http\\S+|www.\\S+\",\"\",tweet)\n",
    "\n",
    "    #Step 2: remove mentions and hashtags\n",
    "    tweet =re.sub(r\"@\\w+|#\\w+\",\"\",tweet)\n",
    "\n",
    "    # Step 3: Remove special characters, numbers, and punctuation\n",
    "    tweet = re.sub(r\"[^a-zA-Z\\s]\", \"\", tweet)\n",
    "    \n",
    "    # Step 4: Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Step 5: Tokenize the text\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Step 7: Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a cleaned sentence\n",
    "    cleaned_tweet = \" \".join(lemmatized_tokens)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c1759c-bfea-498a-8a3c-74ad2d748126",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#1. Fetch Raw Tweets: Assume you have fetched tweets using Tweepy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m tweets \u001b[38;5;241m=\u001b[39m [tweet\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241m.\u001b[39mdata]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
     ]
    }
   ],
   "source": [
    "#1. Fetch Raw Tweets: Assume you have fetched tweets using Tweepy\n",
    "tweets = [tweet.text for tweet in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575c199-ba57-4e9d-81d4-de2b88f63225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
