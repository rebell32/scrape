{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca6c50b-45d8-4292-937f-df72ebf5dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.14.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/codespace/.local/lib/python3.12/site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fc20ec-44d1-4eca-ae3c-4d7aa2601778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6aab9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/workspaces/scrape/.gitignore/.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f662fed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IXcCcUM85jLp6G2g40nMiy6a3\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Test if environment variable is being loaded\n",
    "print(os.getenv(\"API_KEY\"))  # Should print your API key if loaded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f63eec-0626-424f-9c1a-bd0cc382de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# Replace with your credentials\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_secret = os.getenv(\"API_SECRET\")\n",
    "bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "\n",
    "# Authenticate with the Bearer Token\n",
    "try:\n",
    "    client = tweepy.Client(bearer_token=bearer_token)\n",
    "    print(\"Authentication successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during authentication: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e41387-7491-4076-be31-37b24b827f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Tweets:\n",
      "RT @liz_churchill10: To the 17,000 Doctors that are opposing RFK Jr. as the incoming HHS Secretaryâ€¦\n",
      "\n",
      "Please submit to a full forensic finanâ€¦\n",
      "Most active Trading Group ðŸ’¯ \n",
      "https://t.co/EtPiEoOGOu\n",
      "Alerts,Analysis\n",
      "Stocks â˜‘ï¸\n",
      "Options âœ…\n",
      "Day trading âœ…\n",
      "Swing Trading â˜‘ï¸\n",
      "\n",
      "ðŸ”¥\n",
      "$SPY $TSLA $SHOP $AMZN $NVDA $ROKU $EBAY $AMD $BB $DAL https://t.co/3BcdSf6eWy\n",
      "RT @liz_churchill10: To the 17,000 Doctors that are opposing RFK Jr. as the incoming HHS Secretaryâ€¦\n",
      "\n",
      "Please submit to a full forensic finanâ€¦\n",
      "RT @LeadingReport: BREAKING: Bipartisan legislation is being reintroduced by Rep. Cory Mills, Rep. Alexandria Ocasio-Cortez, and others toâ€¦\n",
      "RT @EliteOptions2: The best way to prepare for the future is to learn from the past. \n",
      "\n",
      "Study your trades. Analyze what worked and what didnâ€¦\n",
      "@Haadii661 Hi ðŸ˜ŠDrop hello ðŸ‘‹ to join my #OxNobler  Stocks trading Community group on Telegram \n",
      "ðŸ‘‡ðŸ“©ðŸ‘‡\n",
      "\n",
      "https://t.co/5PYagLTREC\n",
      "RT @CxshHoops: CHRIS BOUCHER IN HIS LAST 10 MINUTES OF PLAY:\n",
      "\n",
      "â€¢ 21 POINTS\n",
      "â€¢ 6 REBOUNDS\n",
      "â€¢ 1 ASSISTS\n",
      "â€¢ 2 STOCKS\n",
      "â€¢ 8/8 FROM THE FIELD\n",
      "â€¢ 5/5 FRâ€¦\n",
      "RT @liz_churchill10: To the 17,000 Doctors that are opposing RFK Jr. as the incoming HHS Secretaryâ€¦\n",
      "\n",
      "Please submit to a full forensic finanâ€¦\n",
      "RT @GoldEagleCom: So far, this week has been calm. Gold Stocksâ€™ Remain Exceptionally Weak Even as Stocks Rise https://t.co/GrHOA0B0Ll\n",
      "RT @LeadingReport: BREAKING: Bipartisan legislation is being reintroduced by Rep. Cory Mills, Rep. Alexandria Ocasio-Cortez, and others toâ€¦\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Search for recent tweets containing the term \"stocks\"\n",
    "    response = client.search_recent_tweets(query=\"stocks\", max_results=10)\n",
    "    \n",
    "    # Display the tweets\n",
    "    if response.data:\n",
    "        print(\"Recent Tweets:\")\n",
    "        for tweet in response.data:\n",
    "            print(tweet.text)\n",
    "    else:\n",
    "        print(\"No tweets found for the given query.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching tweets: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4521b28-4efc-4200-814d-84f9f9359fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ed6f15-3c71-47aa-b357-9f8e3b6f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #for regular expression to clean text\n",
    "import nltk #Natural language Toolkit for text processing\n",
    "from nltk.corpus import stopwords #To remove common, meaningless words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #To break text into tokens (individual words)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc73c9f-bc68-47c8-a00b-4214a102c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #contains words like \"and\",\"the\",\"is\" etc., which are filtered -> don't carry any significant meaning.\n",
    "nltk.download('punkt_tab') #a pre-trained tokenizer model for splitting text into sentences and words\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a9a15d-8abe-48e5-9c53-4f4777c19a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#What the package stopwords contains\n",
    "print(stopwords.words('english'))  # Prints English stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052cc4af-905e-42a4-b5a6-704cb31555b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
      "['Hello, world!', 'NLP is amazing.']\n"
     ]
    }
   ],
   "source": [
    "#how punkt Tokenizer works\n",
    "text = \"Hello, world! NLP is amazing.\"\n",
    "print(word_tokenize(text))  # Outputs: ['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
    "print(sent_tokenize(text))  # Outputs: ['Hello, world!', 'NLP is amazing.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f62ae2-0e0b-4ea6-b2f9-ffe1e0c3c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "#how WordNetLemmatizer works\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, pos=\"v\")  # Specify part of speech (v = verb)\n",
    "print(lemma)  # Output: \"run\"\n",
    "print(lemmatizer.lemmatize(\"better\"))  # Output: \"better\" (default POS is noun)\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Output: \"good\" (adjective POS)\n",
    "#pos=\"n\"-Noun \"v\"=Verb \"a\"=Adjective \"r\"-Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61097b13-5420-4096-9afd-cf20bb2148a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing stopwords and lemmatizer\n",
    "stop_words= set(stopwords.words('english'))\n",
    "lemmantizer=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #Step 1: remove URLs\n",
    "    tweet =re.sub(r\"http\\S+|www.\\S+\",\"\",tweet)\n",
    "\n",
    "    #Step 2: remove mentions and hashtags\n",
    "    tweet =re.sub(r\"@\\w+|#\\w+\",\"\",tweet)\n",
    "\n",
    "    # Step 3: Remove special characters, numbers, and punctuation\n",
    "    tweet = re.sub(r\"[^a-zA-Z\\s]\", \"\", tweet)\n",
    "    \n",
    "    # Step 4: Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Step 5: Tokenize the text\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Step 7: Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a cleaned sentence\n",
    "    cleaned_tweet = \" \".join(lemmatized_tokens)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c1759c-bfea-498a-8a3c-74ad2d748126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Fetch Raw Tweets: Assume you have fetched tweets using Tweepy\n",
    "tweets = [tweet.text for tweet in response.data] #tweet.text, tweet.created_at, tweet.id, tweet.user\n",
    "#response.data contains a list of tweet objects returned by the Twitter API when you make a request using Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f575c199-ba57-4e9d-81d4-de2b88f63225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Clean the Tweets: Use the preprocess_tweet function on each tweet\n",
    "cleaned_tweets =[preprocess_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "989d175a-76f5-40f3-b504-d109dcef750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tweet 1: rt doctor opposing rfk jr incoming hhs secretary please submit full forensic finan\n",
      "Cleaned Tweet 2: active trading group alertsanalysis stock option day trading swing trading spy tsla shop amzn nvda roku ebay amd bb dal\n",
      "Cleaned Tweet 3: rt doctor opposing rfk jr incoming hhs secretary please submit full forensic finan\n",
      "Cleaned Tweet 4: rt breaking bipartisan legislation reintroduced rep cory mill rep alexandria ocasiocortez others\n",
      "Cleaned Tweet 5: rt best way prepare future learn past study trade analyze worked\n",
      "Cleaned Tweet 6: hi drop hello join stock trading community group telegram\n",
      "Cleaned Tweet 7: rt chris boucher last minute play point rebound assist stock field fr\n",
      "Cleaned Tweet 8: rt doctor opposing rfk jr incoming hhs secretary please submit full forensic finan\n",
      "Cleaned Tweet 9: rt far week calm gold stock remain exceptionally weak even stock rise\n",
      "Cleaned Tweet 10: rt breaking bipartisan legislation reintroduced rep cory mill rep alexandria ocasiocortez others\n"
     ]
    }
   ],
   "source": [
    "for i, tweet in enumerate(cleaned_tweets, start=1):\n",
    "    print(f\"Cleaned Tweet {i}: {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa80ddf-d287-43cc-9745-20660552d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#using vaderSentiment analyser \n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1c4ab9-9251-43c2-b676-af946b0f9c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3182, 0.4019, 0.3182, 0.0, 0.6369, 0.0258, 0.34, 0.3182, -0.2247, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#(Optional) Sentiment analysis helps determine whether a tweet expresses a +ve,-ve or neutral sentiment, which is useful for predicted stock movements.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer() #loading model\n",
    "\n",
    "def get_sentiment(tweet):\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)\n",
    "    return sentiment_score['compound'] #Return a score between -1 (negative) and +1 (positive)\n",
    "\n",
    "sentiments = [get_sentiment(tweet) for tweet in cleaned_tweets]\n",
    "print(sentiments) #list of sentiment scores for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1412ec51-5995-4ee3-b047-ce9b1eade6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f41b670-01d9-4d96-81d4-44e20a94e04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1\n",
      "  0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [1 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 3 1\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1\n",
      "  0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      "  1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0\n",
      "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1\n",
      "  0 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 2 0 0 0 0 0 0 0\n",
      "  0 1 1 0]\n",
      " [0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#bag of words technique for feature extraction, converts text data into numerical features based on word frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000)  # Use the top 1000 words\n",
    "X = vectorizer.fit_transform(cleaned_tweets).toarray()  # Converts tweets to a frequency matri\n",
    "print(X)  # Each row is a tweet, and columns are word frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d815128f-b038-40ce-920b-6994bf58e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28448313 0.         0.         0.         0.         0.\n",
      "  0.         0.28448313 0.28448313 0.         0.28448313 0.\n",
      "  0.         0.         0.         0.28448313 0.         0.28448313\n",
      "  0.         0.28448313 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28448313 0.         0.\n",
      "  0.         0.         0.28448313 0.         0.         0.\n",
      "  0.         0.         0.         0.28448313 0.         0.\n",
      "  0.16980036 0.28448313 0.         0.         0.         0.\n",
      "  0.28448313 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.2100554  0.2100554  0.         0.2100554  0.2100554  0.\n",
      "  0.         0.2100554  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.2100554  0.2100554\n",
      "  0.         0.         0.2100554  0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.17856628 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.2100554  0.         0.         0.2100554  0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.2100554\n",
      "  0.         0.         0.2100554  0.2100554  0.13889468 0.\n",
      "  0.         0.2100554  0.         0.         0.53569885 0.2100554\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28448313 0.         0.         0.         0.         0.\n",
      "  0.         0.28448313 0.28448313 0.         0.28448313 0.\n",
      "  0.         0.         0.         0.28448313 0.         0.28448313\n",
      "  0.         0.28448313 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28448313 0.         0.\n",
      "  0.         0.         0.28448313 0.         0.         0.\n",
      "  0.         0.         0.         0.28448313 0.         0.\n",
      "  0.16980036 0.28448313 0.         0.         0.         0.\n",
      "  0.28448313 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.27448625 0.         0.         0.\n",
      "  0.         0.         0.         0.27448625 0.         0.27448625\n",
      "  0.         0.         0.         0.27448625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.27448625 0.27448625\n",
      "  0.         0.         0.27448625 0.         0.         0.27448625\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.27448625 0.         0.54897249 0.         0.         0.\n",
      "  0.14333494 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.31315731\n",
      "  0.         0.         0.31315731 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31315731\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31315731 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.31315731 0.         0.         0.         0.31315731 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.13901435 0.         0.         0.         0.         0.31315731\n",
      "  0.         0.         0.         0.31315731 0.         0.\n",
      "  0.31315731 0.         0.         0.31315731]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.35617798 0.         0.         0.\n",
      "  0.         0.35617798 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30278382 0.35617798 0.         0.35617798 0.\n",
      "  0.35617798 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.23551514 0.\n",
      "  0.         0.         0.35617798 0.         0.30278382 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.30665209 0.         0.         0.         0.30665209 0.\n",
      "  0.         0.30665209 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30665209 0.         0.         0.30665209 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.30665209 0.         0.         0.\n",
      "  0.30665209 0.         0.         0.         0.         0.\n",
      "  0.         0.30665209 0.         0.30665209 0.         0.30665209\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.1361266  0.         0.         0.         0.20276719 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28448313 0.         0.         0.         0.         0.\n",
      "  0.         0.28448313 0.28448313 0.         0.28448313 0.\n",
      "  0.         0.         0.         0.28448313 0.         0.28448313\n",
      "  0.         0.28448313 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.28448313 0.         0.\n",
      "  0.         0.         0.28448313 0.         0.         0.\n",
      "  0.         0.         0.         0.28448313 0.         0.\n",
      "  0.16980036 0.28448313 0.         0.         0.         0.\n",
      "  0.28448313 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30225481 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.30225481 0.30225481 0.30225481\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.30225481 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30225481 0.         0.         0.30225481 0.\n",
      "  0.1341746  0.         0.         0.         0.39971918 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.30225481 0.30225481 0.        ]\n",
      " [0.         0.         0.27448625 0.         0.         0.\n",
      "  0.         0.         0.         0.27448625 0.         0.27448625\n",
      "  0.         0.         0.         0.27448625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.27448625 0.27448625\n",
      "  0.         0.         0.27448625 0.         0.         0.27448625\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.27448625 0.         0.54897249 0.         0.         0.\n",
      "  0.14333494 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#using TF-IDF weighs words based in their importance in the dataset \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf.fit_transform(cleaned_tweets).toarray()\n",
    "print(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3be473c-d3b0-42fd-8c37-f7460b916d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      0.      0.      0.      0.      0.      1.      1.\n",
      "   0.      1.      0.      0.      0.      0.      1.      0.      1.\n",
      "   0.      1.      0.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      0.      0.      0.      1.      0.      0.      0.\n",
      "   0.      0.      0.      1.      0.      0.      1.      1.      0.\n",
      "   0.      0.      0.      1.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.3182]\n",
      " [ 1.      1.      0.      1.      1.      0.      0.      1.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      1.      1.\n",
      "   0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      1.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      1.      0.\n",
      "   0.      1.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      1.      0.      0.      1.\n",
      "   1.      1.      0.      0.      1.      0.      0.      3.      1.\n",
      "   0.      0.      0.      0.      0.4019]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      0.      0.      0.      0.      0.      1.      1.\n",
      "   0.      1.      0.      0.      0.      0.      1.      0.      1.\n",
      "   0.      1.      0.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      0.      0.      0.      1.      0.      0.      0.\n",
      "   0.      0.      0.      1.      0.      0.      1.      1.      0.\n",
      "   0.      0.      0.      1.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.3182]\n",
      " [ 0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      1.      0.      0.      0.      1.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      1.      1.      0.      0.      1.\n",
      "   0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      2.      0.      0.      0.      1.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      1.      0.      0.      1.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      1.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      1.      0.      0.      0.      1.      0.\n",
      "   0.      0.      0.      0.      0.      0.      1.      0.      0.\n",
      "   0.      0.      1.      0.      0.      0.      1.      0.      0.\n",
      "   1.      0.      0.      1.      0.6369]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      1.      0.      0.      0.\n",
      "   0.      1.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      1.      1.      0.      1.      0.\n",
      "   1.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      1.      0.      0.      0.      1.      0.      1.      0.\n",
      "   0.      0.      0.      0.      0.0258]\n",
      " [ 0.      0.      0.      0.      0.      0.      1.      0.      0.\n",
      "   0.      1.      0.      0.      1.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      1.      0.      0.\n",
      "   1.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      1.      0.      0.      0.      1.      0.      0.\n",
      "   0.      0.      0.      0.      1.      0.      1.      0.      1.\n",
      "   0.      0.      0.      0.      0.      0.      1.      0.      0.\n",
      "   0.      1.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.34  ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      0.      0.      0.      0.      0.      1.      1.\n",
      "   0.      1.      0.      0.      0.      0.      1.      0.      1.\n",
      "   0.      1.      0.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      0.      0.      0.      1.      0.      0.      0.\n",
      "   0.      0.      0.      1.      0.      0.      1.      1.      0.\n",
      "   0.      0.      0.      1.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.3182]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      1.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      1.      1.      1.      0.      0.      0.\n",
      "   0.      0.      0.      1.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      1.      0.      0.      1.      0.      1.      0.      0.\n",
      "   0.      2.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      1.      1.      0.     -0.2247]\n",
      " [ 0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      1.      0.      0.      0.      1.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      1.      1.      0.      0.      1.\n",
      "   0.      0.      1.      0.      0.      0.      0.      0.      0.\n",
      "   1.      0.      2.      0.      0.      0.      1.      0.      0.\n",
      "   0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   0.      0.      0.      0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine features\n",
    "X_combined = np.hstack((X, np.array(sentiments).reshape(-1, 1)))\n",
    "print(X_combined)  # Each row includes text features + sentiment score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8444cf-73d3-459e-8f07-1c668e943bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.2.51)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.31 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (5.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2024.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (3.17.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: six>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/codespace/.local/lib/python3.12/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0401536-4cac-4e5b-b456-1bcd0a01d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open     Volume\n",
      "Ticker            TSLA        TSLA        TSLA        TSLA       TSLA\n",
      "Date                                                                 \n",
      "2024-01-02  248.419998  251.250000  244.410004  250.080002  104654200\n",
      "2024-01-03  238.449997  245.679993  236.320007  244.979996  121082600\n",
      "2024-01-04  237.929993  242.699997  237.729996  239.250000  102629300\n",
      "2024-01-05  237.490005  240.119995  234.899994  236.860001   92379400\n",
      "2024-01-08  240.449997  241.250000  235.300003  236.139999   85166600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "#Fetch historical stock data\n",
    "stock_data = yf.download('TSLA', start='2024-01-01', end='2024-12-31')\n",
    "\n",
    "#Define stock movement labels (1=Up, 0=Down)\n",
    "print(stock_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9442a25-ea91-464a-b2d2-1be0fe89a9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        Open Movement\n",
      "Ticker            TSLA        TSLA         \n",
      "Date                                       \n",
      "2024-01-02  248.419998  250.080002        0\n",
      "2024-01-03  238.449997  244.979996        0\n",
      "2024-01-04  237.929993  239.250000        0\n",
      "2024-01-05  237.490005  236.860001        1\n",
      "2024-01-08  240.449997  236.139999        1\n"
     ]
    }
   ],
   "source": [
    "#if closing price is greater than opening price, stock=Up=1\n",
    "stock_data['Movement']= (stock_data['Close']>stock_data['Open']).astype(int)\n",
    "print(stock_data[['Close','Open','Movement']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3071009a-7eb2-4bd9-932a-50a475841010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp                     text        date\n",
      "0 2024-10-10 14:30:00    Stock market booming!  2024-10-10\n",
      "1 2024-10-11 10:00:00  Tesla stock looks weak.  2024-10-11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = [\n",
    "    {\"timestamp\": \"2024-10-10 14:30:00\", \"text\": \"Stock market booming!\"},\n",
    "    {\"timestamp\": \"2024-10-11 10:00:00\", \"text\": \"Tesla stock looks weak.\"},\n",
    "]\n",
    "\n",
    "tweet_data = pd.DataFrame(tweets)\n",
    "tweet_data['timestamp'] = pd.to_datetime(tweet_data['timestamp'])\n",
    "\n",
    "tweet_data['date'] = tweet_data['timestamp'].dt.date\n",
    "print(tweet_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b445ac83-da6c-4a57-acd7-0d3275919b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data['date'] = stock_data.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e7fb15c-daa0-4740-af2a-f984147e4f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tweet_data.index.nlevels)  # Should be 1\n",
    "print(stock_data.index.nlevels)  # Likely 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62b6f5f5-445f-4ea9-b408-2b85a3c7f5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([2024-10-10, 2024-10-11], dtype='object', name='date')\n",
      "Index([2024-01-02, 2024-01-03, 2024-01-04, 2024-01-05, 2024-01-08, 2024-01-09,\n",
      "       2024-01-10, 2024-01-11, 2024-01-12, 2024-01-16,\n",
      "       ...\n",
      "       2024-12-16, 2024-12-17, 2024-12-18, 2024-12-19, 2024-12-20, 2024-12-23,\n",
      "       2024-12-24, 2024-12-26, 2024-12-27, 2024-12-30],\n",
      "      dtype='object', name='date', length=251)\n"
     ]
    }
   ],
   "source": [
    "print(tweet_data.index)  # Check the index of tweet_data\n",
    "print(stock_data.index)  # Check the index of stock_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e530b7e3-e3ed-4d65-8ddf-b3ae75c7d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tweet_data\n",
    "tweet_data.index = pd.to_datetime(tweet_data.index).date\n",
    "\n",
    "# For stock_data\n",
    "stock_data.index = pd.to_datetime(stock_data.index).date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f3a12b7-b85b-4f78-8daf-8b0727b73982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the name 'date' to the indices\n",
    "tweet_data.index.name = 'date'\n",
    "stock_data.index.name = 'date'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ca07f917-af10-4401-a5a1-a672158a01fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Not allowed to merge between different levels. (1 levels on the left, 2 on the right)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Join the two DataFrames on the 'date' index\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[43mtweet_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMovement\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/frame.py:10757\u001b[0m, in \u001b[0;36mDataFrame.join\u001b[0;34m(self, other, on, how, lsuffix, rsuffix, sort, validate)\u001b[0m\n\u001b[1;32m  10747\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m  10748\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m merge(\n\u001b[1;32m  10749\u001b[0m             \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10750\u001b[0m             other,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10755\u001b[0m             validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m  10756\u001b[0m         )\n\u001b[0;32m> 10757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m  10763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m  10764\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlsuffix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrsuffix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10765\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10767\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10768\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m  10769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/merge.py:784\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m!=\u001b[39m _right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels:\n\u001b[1;32m    779\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot allowed to merge between different levels. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m levels on the left, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on the right)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m     )\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_left_right_on(left_on, right_on)\n\u001b[1;32m    788\u001b[0m (\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     right_drop,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n",
      "\u001b[0;31mMergeError\u001b[0m: Not allowed to merge between different levels. (1 levels on the left, 2 on the right)"
     ]
    }
   ],
   "source": [
    "# Join the two DataFrames on the 'date' index\n",
    "merged_data = tweet_data.join(stock_data[['Movement']], how='inner')\n",
    "print(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c9090dd-614b-431b-a721-4389fe21920d",
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Not allowed to merge between different levels. (1 levels on the left, 2 on the right)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m stock_data \u001b[38;5;241m=\u001b[39m stock_data\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Perform the merge on the 'date' column\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m merged_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweet_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstock_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMovement\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minner\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_data)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[1;32m    156\u001b[0m         left_df,\n\u001b[1;32m    157\u001b[0m         right_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/reshape/merge.py:784\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m!=\u001b[39m _right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels:\n\u001b[1;32m    779\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    780\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot allowed to merge between different levels. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    781\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_left\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m levels on the left, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    782\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_right\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on the right)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    783\u001b[0m     )\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(msg)\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_on, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_left_right_on(left_on, right_on)\n\u001b[1;32m    788\u001b[0m (\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    793\u001b[0m     right_drop,\n\u001b[1;32m    794\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n",
      "\u001b[0;31mMergeError\u001b[0m: Not allowed to merge between different levels. (1 levels on the left, 2 on the right)"
     ]
    }
   ],
   "source": [
    "# Reset index to make 'date' a regular column\n",
    "tweet_data = tweet_data.reset_index()\n",
    "stock_data = stock_data.reset_index()\n",
    "\n",
    "# Perform the merge on the 'date' column\n",
    "merged_data = pd.merge(tweet_data, stock_data[['date', 'Movement']], on='date', how='inner')\n",
    "print(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee270581-da39-4006-9a4d-5abba637789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.date'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tweet_data.iloc[0]['date']))  # Should output <class 'datetime.date'>\n",
    "print(type(stock_data.iloc[0]['date']))  # Should output <class 'datetime.date'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b025eaa-5995-4267-8237-699f6edfebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = stock_data.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79c91d1f-6abc-4ead-a8c9-ec6e7a165837",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data['date'] = pd.to_datetime(stock_data['date']).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d1d617f-08cb-4ef8-96a0-95d36278b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.date'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tweet_data.iloc[0]['date']))  # Should be <class 'datetime.date'>\n",
    "print(type(stock_data.iloc[0]['date']))  # Should now also be <class 'datetime.date'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87ca18ee-6a8a-4d64-9736-0e026dabc5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{datetime.date(2024, 10, 10), datetime.date(2024, 10, 11)}\n"
     ]
    }
   ],
   "source": [
    "print(set(tweet_data['date']).intersection(set(stock_data['date'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145387d5-2db1-452c-becd-2a5aa5a17539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
