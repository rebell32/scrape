{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca6c50b-45d8-4292-937f-df72ebf5dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.14.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/codespace/.local/lib/python3.12/site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fc20ec-44d1-4eca-ae3c-4d7aa2601778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6aab9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/workspaces/scrape/.gitignore/.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f662fed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IXcCcUM85jLp6G2g40nMiy6a3\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Test if environment variable is being loaded\n",
    "print(os.getenv(\"API_KEY\"))  # Should print your API key if loaded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f63eec-0626-424f-9c1a-bd0cc382de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# Replace with your credentials\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_secret = os.getenv(\"API_SECRET\")\n",
    "bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "\n",
    "# Authenticate with the Bearer Token\n",
    "try:\n",
    "    client = tweepy.Client(bearer_token=bearer_token)\n",
    "    print(\"Authentication successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during authentication: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e41387-7491-4076-be31-37b24b827f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Tweets:\n",
      "@StockMKTNewz Oh, interesting! Could impact stocks\n",
      "RT @MiatoTechnolog: 🎉 Miato New Year Super Sale!🎉  \n",
      "\n",
      "Own the reliable HP EliteBook 840 (6th Gen)\n",
      "✨ 14-inch Display |8GB RAM| 256GB SSD\n",
      "Now…\n",
      "Gold has outshined stocks in the 21st century. The DJCI Gold TR index outperformed the S&amp;P 500 TR index not just in 2024 but over the entire period since Dec 31. https://t.co/Z6vXzSTNCL\n",
      "RT @wallstreetbets: Cardano’s $ADA 11.5% surge is the crypto equivalent of a rollercoaster ride: thrilling, unpredictable, and you might ju…\n",
      "RT @RepDeluzio: Good morning. It should be illegal for members of Congress to trade stocks. Let’s ban this swampy nonsense.\n",
      "RT @ericnuttall: It’s refreshing to hear such an energy-literate politician, especially when that person is likely to become Canada’s next…\n",
      "U.S. stocks are showing weakness, and the dollar’s surge adds to market uncertainty. $SOL/ETH is reversing, and AI projects are seeing a rush of capital. Interesting shifts happening with future listings like $BIO from DeSci on #BingX.\n",
      "#CryptoNews #MarketTrends #Stocks #AI #BingX https://t.co/qjHUIdIIWt https://t.co/rHDa0YcDYf\n",
      "RT @MiatoTechnolog: 🎉 Miato New Year Super Sale!🎉  \n",
      "\n",
      "Own the reliable HP EliteBook 840 (6th Gen)\n",
      "✨ 14-inch Display |8GB RAM| 256GB SSD\n",
      "Now…\n",
      "I know everyone wants to buy the dip! It is the most natural thing to do! You can certainly do that with stocks over long period of time. IMO, the bottom is not in yet! If you are buying calls, be prepared to lose it all! Not financial advice!\n",
      "\n",
      "$QQQ $NVDA $TSLA $AMD $GOOGL $AAPL… https://t.co/9POaC9335O https://t.co/v6GQG2ydiF\n",
      "RT @equialpha: Top 10 alcohol stocks in India 🍻\n",
      "\n",
      "1. United Spirits \n",
      "2. United Breweries \n",
      "3. Radico Khaitan \n",
      "4. Globus Spirits  \n",
      "5. Sula Vin…\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Search for recent tweets containing the term \"stocks\"\n",
    "    response = client.search_recent_tweets(query=\"stocks\", max_results=10)\n",
    "    \n",
    "    # Display the tweets\n",
    "    if response.data:\n",
    "        print(\"Recent Tweets:\")\n",
    "        for tweet in response.data:\n",
    "            print(tweet.text)\n",
    "    else:\n",
    "        print(\"No tweets found for the given query.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching tweets: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4521b28-4efc-4200-814d-84f9f9359fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ed6f15-3c71-47aa-b357-9f8e3b6f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #for regular expression to clean text\n",
    "import nltk #Natural language Toolkit for text processing\n",
    "from nltk.corpus import stopwords #To remove common, meaningless words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #To break text into tokens (individual words)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc73c9f-bc68-47c8-a00b-4214a102c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #contains words like \"and\",\"the\",\"is\" etc., which are filtered -> don't carry any significant meaning.\n",
    "nltk.download('punkt_tab') #a pre-trained tokenizer model for splitting text into sentences and words\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a9a15d-8abe-48e5-9c53-4f4777c19a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#What the package stopwords contains\n",
    "print(stopwords.words('english'))  # Prints English stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052cc4af-905e-42a4-b5a6-704cb31555b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
      "['Hello, world!', 'NLP is amazing.']\n"
     ]
    }
   ],
   "source": [
    "#how punkt Tokenizer works\n",
    "text = \"Hello, world! NLP is amazing.\"\n",
    "print(word_tokenize(text))  # Outputs: ['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
    "print(sent_tokenize(text))  # Outputs: ['Hello, world!', 'NLP is amazing.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f62ae2-0e0b-4ea6-b2f9-ffe1e0c3c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "#how WordNetLemmatizer works\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, pos=\"v\")  # Specify part of speech (v = verb)\n",
    "print(lemma)  # Output: \"run\"\n",
    "print(lemmatizer.lemmatize(\"better\"))  # Output: \"better\" (default POS is noun)\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Output: \"good\" (adjective POS)\n",
    "#pos=\"n\"-Noun \"v\"=Verb \"a\"=Adjective \"r\"-Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61097b13-5420-4096-9afd-cf20bb2148a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing stopwords and lemmatizer\n",
    "stop_words= set(stopwords.words('english'))\n",
    "lemmantizer=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #Step 1: remove URLs\n",
    "    tweet =re.sub(r\"http\\S+|www.\\S+\",\"\",tweet)\n",
    "\n",
    "    #Step 2: remove mentions and hashtags\n",
    "    tweet =re.sub(r\"@\\w+|#\\w+\",\"\",tweet)\n",
    "\n",
    "    # Step 3: Remove special characters, numbers, and punctuation\n",
    "    tweet = re.sub(r\"[^a-zA-Z\\s]\", \"\", tweet)\n",
    "    \n",
    "    # Step 4: Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Step 5: Tokenize the text\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Step 7: Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a cleaned sentence\n",
    "    cleaned_tweet = \" \".join(lemmatized_tokens)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c1759c-bfea-498a-8a3c-74ad2d748126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Fetch Raw Tweets: Assume you have fetched tweets using Tweepy\n",
    "tweets = [tweet.text for tweet in response.data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f575c199-ba57-4e9d-81d4-de2b88f63225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Clean the Tweets: Use the preprocess_tweet function on each tweet\n",
    "cleaned_tweets =[preprocess_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "989d175a-76f5-40f3-b504-d109dcef750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tweet 1: oh interesting could impact stock\n",
      "Cleaned Tweet 2: rt miato new year super sale reliable hp elitebook th gen inch display gb ram gb ssd\n",
      "Cleaned Tweet 3: gold outshined stock st century djci gold tr index outperformed sampp tr index entire period since dec\n",
      "Cleaned Tweet 4: rt cardanos ada surge crypto equivalent rollercoaster ride thrilling unpredictable might ju\n",
      "Cleaned Tweet 5: rt good morning illegal member congress trade stock let ban swampy nonsense\n",
      "Cleaned Tweet 6: rt refreshing hear energyliterate politician especially person likely become canada next\n",
      "Cleaned Tweet 7: u stock showing weakness dollar surge add market uncertainty soleth reversing ai project seeing rush capital interesting shift happening future listing like bio desci\n",
      "Cleaned Tweet 8: rt miato new year super sale reliable hp elitebook th gen inch display gb ram gb ssd\n",
      "Cleaned Tweet 9: know everyone want buy dip natural thing certainly stock long period time imo bottom yet buying call prepared lose financial advice qqq nvda tsla amd googl aapl\n",
      "Cleaned Tweet 10: rt top alcohol stock india united spirit united brewery radico khaitan globus spirit sula vin\n"
     ]
    }
   ],
   "source": [
    "for i, tweet in enumerate(cleaned_tweets, start=1):\n",
    "    print(f\"Cleaned Tweet {i}: {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa80ddf-d287-43cc-9745-20660552d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#using vaderSentiment analyser\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1c4ab9-9251-43c2-b676-af946b0f9c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4019, 0.5994, 0.0, 0.4767, -0.7906, 0.0, 0.0, 0.5994, 0.5267, 0.8316]\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(tweet):\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)\n",
    "    return sentiment_score['compound'] #Return a score between -1 (negative) and +1 (positive)\n",
    "\n",
    "sentiments = [get_sentiment(tweet) for tweet in cleaned_tweets]\n",
    "print(sentiments) #list of sentiment scores for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1412ec51-5995-4ee3-b047-ce9b1eade6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f41b670-01d9-4d96-81d4-44e20a94e04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [1 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000)  # Use the top 1000 words\n",
    "X = vectorizer.fit_transform(cleaned_tweets).toarray()  # X is a matrix of features\n",
    "print(X)  # Each row is a tweet, and columns are word frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d815128f-b038-40ce-920b-6994bf58e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.2331339  0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.2331339  0.        ]\n",
      " [0.19607527 0.         0.         ... 0.         0.         0.19607527]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf.fit_transform(cleaned_tweets).toarray()\n",
    "print(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3be473c-d3b0-42fd-8c37-f7460b916d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     0.     0.     ... 0.     0.     0.4019]\n",
      " [0.     0.     0.     ... 1.     0.     0.5994]\n",
      " [0.     0.     0.     ... 0.     0.     0.    ]\n",
      " ...\n",
      " [0.     0.     0.     ... 1.     0.     0.5994]\n",
      " [1.     0.     0.     ... 0.     1.     0.5267]\n",
      " [0.     0.     0.     ... 0.     0.     0.8316]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine features\n",
    "X_combined = np.hstack((X, np.array(sentiments).reshape(-1, 1)))\n",
    "print(X_combined)  # Each row includes text features + sentiment score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8444cf-73d3-459e-8f07-1c668e943bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
