{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca6c50b-45d8-4292-937f-df72ebf5dbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.14.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/codespace/.local/lib/python3.12/site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3,>=2.27.0->tweepy) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5fc20ec-44d1-4eca-ae3c-4d7aa2601778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6aab9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"/workspaces/scrape/.gitignore/.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f662fed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IXcCcUM85jLp6G2g40nMiy6a3\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Test if environment variable is being loaded\n",
    "print(os.getenv(\"API_KEY\"))  # Should print your API key if loaded correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09f63eec-0626-424f-9c1a-bd0cc382de32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication successful!\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# Replace with your credentials\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_secret = os.getenv(\"API_SECRET\")\n",
    "bearer_token = os.getenv(\"BEARER_TOKEN\")\n",
    "\n",
    "# Authenticate with the Bearer Token\n",
    "try:\n",
    "    client = tweepy.Client(bearer_token=bearer_token)\n",
    "    print(\"Authentication successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during authentication: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e41387-7491-4076-be31-37b24b827f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent Tweets:\n",
      "RT @ahmedyoussefbad: The market is improving, $CLOV has gained 30% I will release the 2025 stock portfolio plan every day through WhatsApp.…\n",
      "@NDIDI_GRAM Also follow this guy @qudsiakhan100\n",
      "\n",
      "His recent trades were so accurate, I followed his advice and bought the same stocks and made $95k profit.🥰\n",
      "\n",
      "I think everyone should try to follow. It's amazing.👍🏻\n",
      "Star Fashion Culture Holdings Ltd ($STFS) is a China-based advertising agency specializing in communication services. Their stock surged 24.25% today! 📈 #Advertising #ChinaBusiness #CommunicationServices #DayTrading #AlgoTrading #Stocks https://t.co/CxvRtRokhe https://t.co/GV9JKtLRWt\n",
      "4752 (株)昭和システムエンジニアリング\n",
      "+26(1.79%) \n",
      "東証が信用取引に関する臨時措置を解除\n",
      "pts爆上げ！\n",
      "https://t.co/EAJxp6Wvh3\n",
      "RT @Alphamojo1: 15 Great Stocks!📊 https://t.co/RsHFvhaS9z\n",
      "I’m currently building out a private discord and won’t be released until I hit 1k followers.\n",
      "\n",
      "It’s going to include channels for:\n",
      "- Trading education (patterns, indicators, EW)\n",
      "- Fundamental analysis education (how to pick good stocks\n",
      "reading balance and income sheets)        -… https://t.co/Y0vWiI8Cgl\n",
      "RT @MoneyWisdom_: \"If you have trouble imagining a 20% loss in the stock market, you shouldn’t be in stocks.\"\n",
      "\n",
      "— John Bogle\n",
      "RT @TheBuckYouWill: NOTHING TO SEE HERE - JUST A SHADY MOTHERFUCKER.\n",
      "\n",
      "—&gt; All Canadians should be DEEPLY concerned about this. &lt;—\n",
      "\n",
      "Trudeau w…\n",
      "@shermantank1717 Make sure you join (@CryptoZachLA) mentorship program so you don't miss investment ideas on #Trades #Stocks and #Crypto, DM Hello 👋 let get you started.\n",
      "👇📥👇\n",
      "\n",
      "https://t.co/8AsAShbWGL\n",
      "Texas Roadhouse outperforms the AAA list tech stocks like apple and Netflix.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Search for recent tweets containing the term \"stocks\"\n",
    "    response = client.search_recent_tweets(query=\"stocks\", max_results=10)\n",
    "    \n",
    "    # Display the tweets\n",
    "    if response.data:\n",
    "        print(\"Recent Tweets:\")\n",
    "        for tweet in response.data:\n",
    "            print(tweet.text)\n",
    "    else:\n",
    "        print(\"No tweets found for the given query.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching tweets: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4521b28-4efc-4200-814d-84f9f9359fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0ed6f15-3c71-47aa-b357-9f8e3b6f1733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #for regular expression to clean text\n",
    "import nltk #Natural language Toolkit for text processing\n",
    "from nltk.corpus import stopwords #To remove common, meaningless words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #To break text into tokens (individual words)\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cc73c9f-bc68-47c8-a00b-4214a102c848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') #contains words like \"and\",\"the\",\"is\" etc., which are filtered -> don't carry any significant meaning.\n",
    "nltk.download('punkt_tab') #a pre-trained tokenizer model for splitting text into sentences and words\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a9a15d-8abe-48e5-9c53-4f4777c19a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#What the package stopwords contains\n",
    "print(stopwords.words('english'))  # Prints English stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "052cc4af-905e-42a4-b5a6-704cb31555b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
      "['Hello, world!', 'NLP is amazing.']\n"
     ]
    }
   ],
   "source": [
    "#how punkt Tokenizer works\n",
    "text = \"Hello, world! NLP is amazing.\"\n",
    "print(word_tokenize(text))  # Outputs: ['Hello', ',', 'world', '!', 'NLP', 'is', 'amazing', '.']\n",
    "print(sent_tokenize(text))  # Outputs: ['Hello, world!', 'NLP is amazing.']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8f62ae2-0e0b-4ea6-b2f9-ffe1e0c3c08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "better\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "#how WordNetLemmatizer works\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, pos=\"v\")  # Specify part of speech (v = verb)\n",
    "print(lemma)  # Output: \"run\"\n",
    "print(lemmatizer.lemmatize(\"better\"))  # Output: \"better\" (default POS is noun)\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Output: \"good\" (adjective POS)\n",
    "#pos=\"n\"-Noun \"v\"=Verb \"a\"=Adjective \"r\"-Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61097b13-5420-4096-9afd-cf20bb2148a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing stopwords and lemmatizer\n",
    "stop_words= set(stopwords.words('english'))\n",
    "lemmantizer=WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    #Step 1: remove URLs\n",
    "    tweet =re.sub(r\"http\\S+|www.\\S+\",\"\",tweet)\n",
    "\n",
    "    #Step 2: remove mentions and hashtags\n",
    "    tweet =re.sub(r\"@\\w+|#\\w+\",\"\",tweet)\n",
    "\n",
    "    # Step 3: Remove special characters, numbers, and punctuation\n",
    "    tweet = re.sub(r\"[^a-zA-Z\\s]\", \"\", tweet)\n",
    "    \n",
    "    # Step 4: Convert text to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Step 5: Tokenize the text\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Step 7: Lemmatize the tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Join tokens back into a cleaned sentence\n",
    "    cleaned_tweet = \" \".join(lemmatized_tokens)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42c1759c-bfea-498a-8a3c-74ad2d748126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Fetch Raw Tweets: Assume you have fetched tweets using Tweepy\n",
    "tweets = [tweet.text for tweet in response.data] #tweet.text, tweet.created_at, tweet.id, tweet.user\n",
    "#response.data contains a list of tweet objects returned by the Twitter API when you make a request using Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f575c199-ba57-4e9d-81d4-de2b88f63225",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Clean the Tweets: Use the preprocess_tweet function on each tweet\n",
    "cleaned_tweets =[preprocess_tweet(tweet) for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "989d175a-76f5-40f3-b504-d109dcef750f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Tweet 1: rt market improving clov gained release stock portfolio plan every day whatsapp\n",
      "Cleaned Tweet 2: also follow guy recent trade accurate followed advice bought stock made k profit think everyone try follow amazing\n",
      "Cleaned Tweet 3: star fashion culture holding ltd stfs chinabased advertising agency specializing communication service stock surged today\n",
      "Cleaned Tweet 4: pt\n",
      "Cleaned Tweet 5: rt great stock\n",
      "Cleaned Tweet 6: im currently building private discord wont released hit k follower going include channel trading education pattern indicator ew fundamental analysis education pick good stock reading balance income sheet\n",
      "Cleaned Tweet 7: rt trouble imagining loss stock market shouldnt stock john bogle\n",
      "Cleaned Tweet 8: rt nothing see shady motherfucker gt canadian deeply concerned lt trudeau w\n",
      "Cleaned Tweet 9: make sure join mentorship program dont miss investment idea dm hello let get started\n",
      "Cleaned Tweet 10: texas roadhouse outperforms aaa list tech stock like apple netflix\n"
     ]
    }
   ],
   "source": [
    "for i, tweet in enumerate(cleaned_tweets, start=1):\n",
    "    print(f\"Cleaned Tweet {i}: {tweet}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa80ddf-d287-43cc-9745-20660552d574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->vaderSentiment) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "#using vaderSentiment analyser \n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba1c4ab9-9251-43c2-b676-af946b0f9c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6597, 0.7717, 0.0, 0.0, 0.6249, 0.0516, -0.6124, 0.6969, 0.6052, 0.3612]\n"
     ]
    }
   ],
   "source": [
    "#(Optional) Sentiment analysis helps determine whether a tweet expresses a +ve,-ve or neutral sentiment, which is useful for predicted stock movements.\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer() #loading model\n",
    "\n",
    "def get_sentiment(tweet):\n",
    "    sentiment_score = analyzer.polarity_scores(tweet)\n",
    "    return sentiment_score['compound'] #Return a score between -1 (negative) and +1 (positive)\n",
    "\n",
    "sentiments = [get_sentiment(tweet) for tweet in cleaned_tweets]\n",
    "print(sentiments) #list of sentiment scores for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1412ec51-5995-4ee3-b047-ce9b1eade6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f41b670-01d9-4d96-81d4-44e20a94e04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 1 0]\n",
      " [0 1 0 ... 1 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#bag of words technique for feature extraction, converts text data into numerical features based on word frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer(max_features=1000)  # Use the top 1000 words\n",
    "X = vectorizer.fit_transform(cleaned_tweets).toarray()  # Converts tweets to a frequency matri\n",
    "print(X)  # Each row is a tweet, and columns are word frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d815128f-b038-40ce-920b-6994bf58e9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.31012419 0.        ]\n",
      " [0.         0.23416175 0.         ... 0.23416175 0.         0.        ]\n",
      " [0.         0.         0.26502165 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.32901829 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#using TF-IDF weighs words based in their importance in the dataset \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf.fit_transform(cleaned_tweets).toarray()\n",
    "print(X_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3be473c-d3b0-42fd-8c37-f7460b916d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.     0.     0.     ... 1.     0.     0.6597]\n",
      " [0.     1.     0.     ... 0.     0.     0.7717]\n",
      " [0.     0.     1.     ... 0.     0.     0.    ]\n",
      " ...\n",
      " [0.     0.     0.     ... 0.     0.     0.6969]\n",
      " [0.     0.     0.     ... 0.     0.     0.6052]\n",
      " [1.     0.     0.     ... 0.     0.     0.3612]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine features\n",
    "X_combined = np.hstack((X, np.array(sentiments).reshape(-1, 1)))\n",
    "print(X_combined)  # Each row includes text features + sentiment score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8444cf-73d3-459e-8f07-1c668e943bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.2.51)\n",
      "Requirement already satisfied: pandas>=1.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.31 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: lxml>=4.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (5.3.0)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (4.3.6)\n",
      "Requirement already satisfied: pytz>=2022.5 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (2024.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (3.17.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in /home/codespace/.local/lib/python3.12/site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: html5lib>=1.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from yfinance) (1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/codespace/.local/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
      "Requirement already satisfied: six>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /home/codespace/.local/lib/python3.12/site-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0401536-4cac-4e5b-b456-1bcd0a01d9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open     Volume\n",
      "Ticker            TSLA        TSLA        TSLA        TSLA       TSLA\n",
      "Date                                                                 \n",
      "2024-01-02  248.419998  251.250000  244.410004  250.080002  104654200\n",
      "2024-01-03  238.449997  245.679993  236.320007  244.979996  121082600\n",
      "2024-01-04  237.929993  242.699997  237.729996  239.250000  102629300\n",
      "2024-01-05  237.490005  240.119995  234.899994  236.860001   92379400\n",
      "2024-01-08  240.449997  241.250000  235.300003  236.139999   85166600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "#Fetch historical stock data\n",
    "stock_data = yf.download('TSLA', start='2024-01-01', end='2024-12-31')\n",
    "\n",
    "#Define stock movement labels (1=Up, 0=Down)\n",
    "print(stock_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9442a25-ea91-464a-b2d2-1be0fe89a9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        Open Movement\n",
      "Ticker            TSLA        TSLA         \n",
      "Date                                       \n",
      "2024-01-02  248.419998  250.080002        0\n",
      "2024-01-03  238.449997  244.979996        0\n",
      "2024-01-04  237.929993  239.250000        0\n",
      "2024-01-05  237.490005  236.860001        1\n",
      "2024-01-08  240.449997  236.139999        1\n"
     ]
    }
   ],
   "source": [
    "#if closing price is greater than opening price, stock=Up=1\n",
    "stock_data['Movement']= (stock_data['Close']>stock_data['Open']).astype(int)\n",
    "print(stock_data[['Close','Open','Movement']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3071009a-7eb2-4bd9-932a-50a475841010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp                     text        date\n",
      "0 2024-10-10 14:30:00    Stock market booming!  2024-10-10\n",
      "1 2024-10-11 10:00:00  Tesla stock looks weak.  2024-10-11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweets = [\n",
    "    {\"timestamp\": \"2024-10-10 14:30:00\", \"text\": \"Stock market booming!\"},\n",
    "    {\"timestamp\": \"2024-10-11 10:00:00\", \"text\": \"Tesla stock looks weak.\"},\n",
    "]\n",
    "\n",
    "tweet_data = pd.DataFrame(tweets)\n",
    "tweet_data['timestamp'] = pd.to_datetime(tweet_data['timestamp'])\n",
    "\n",
    "tweet_data['date'] = tweet_data['timestamp'].dt.date\n",
    "print(tweet_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b445ac83-da6c-4a57-acd7-0d3275919b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data['date'] = stock_data.index.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e7fb15c-daa0-4740-af2a-f984147e4f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(tweet_data.index.nlevels)  # Should be 1\n",
    "print(stock_data.index.nlevels)  # Likely 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e530b7e3-e3ed-4d65-8ddf-b3ae75c7d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tweet_data\n",
    "tweet_data.index = pd.to_datetime(tweet_data.index).date\n",
    "\n",
    "# For stock_data\n",
    "stock_data.index = pd.to_datetime(stock_data.index).date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f3a12b7-b85b-4f78-8daf-8b0727b73982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the name 'date' to the indices\n",
    "tweet_data.index.name = 'date'\n",
    "stock_data.index.name = 'date'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "145387d5-2db1-452c-becd-2a5aa5a17539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([1970-01-01, 1970-01-01], dtype='object', name='date')\n",
      "Index([2024-01-02, 2024-01-03, 2024-01-04, 2024-01-05, 2024-01-08, 2024-01-09,\n",
      "       2024-01-10, 2024-01-11, 2024-01-12, 2024-01-16,\n",
      "       ...\n",
      "       2024-12-16, 2024-12-17, 2024-12-18, 2024-12-19, 2024-12-20, 2024-12-23,\n",
      "       2024-12-24, 2024-12-26, 2024-12-27, 2024-12-30],\n",
      "      dtype='object', name='date', length=251)\n"
     ]
    }
   ],
   "source": [
    "print(tweet_data.index)  # Check the index of tweet_data\n",
    "print(stock_data.index)  # Check the index of stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48edd9e0-78ad-4c20-8759-ea78d5665538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([2024-01-02, 2024-01-03, 2024-01-04, 2024-01-05, 2024-01-08, 2024-01-09,\n",
      "       2024-01-10, 2024-01-11, 2024-01-12, 2024-01-16,\n",
      "       ...\n",
      "       2024-12-16, 2024-12-17, 2024-12-18, 2024-12-19, 2024-12-20, 2024-12-23,\n",
      "       2024-12-24, 2024-12-26, 2024-12-27, 2024-12-30],\n",
      "      dtype='object', length=251)\n"
     ]
    }
   ],
   "source": [
    "print(stock_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1e15741-d0e7-4077-a312-1bdad36e05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index to datetime.date\n",
    "stock_data.index = pd.to_datetime(stock_data.index).date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa15d065-ae48-48ba-8359-8d1d2716623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tweet_data index to datetime.date\n",
    "tweet_data.index = pd.to_datetime(tweet_data.index).date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a347a629-8e7d-490b-89f9-9b51a0297b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datetime.date'>\n",
      "<class 'datetime.date'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tweet_data.index[0]))  # Should be datetime64[ns] or datetime.date\n",
    "print(type(stock_data.index[0]))  # Should also be datetime.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9090dd-614b-431b-a721-4389fe21920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index to make 'date' a regular column\n",
    "tweet_data = tweet_data.reset_index()\n",
    "stock_data = stock_data.reset_index()\n",
    "\n",
    "# Perform the merge on the 'date' column\n",
    "merged_data = pd.merge(tweet_data, stock_data[['date', 'Movement']], on='date', how='inner')\n",
    "print(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87252537-0053-48f1-8dbb-b6733ac891b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{datetime.date(2024, 10, 10), datetime.date(2024, 10, 11)}\n"
     ]
    }
   ],
   "source": [
    "print(set(tweet_data['date']).intersection(set(stock_data['date'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a87fe421-c927-4fff-be3d-aba21e0618b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price         date Movement\n",
      "Ticker                     \n",
      "0       2024-01-02        0\n",
      "1       2024-01-03        0\n",
      "2       2024-01-04        0\n",
      "3       2024-01-05        1\n",
      "4       2024-01-08        1\n",
      "MultiIndex([(    'date', ''),\n",
      "            ('Movement', '')],\n",
      "           names=['Price', 'Ticker'])\n",
      "RangeIndex(start=0, stop=251, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(stock_data[['date', 'Movement']].head())\n",
    "print(stock_data[['date', 'Movement']].columns)\n",
    "print(stock_data[['date', 'Movement']].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba2273dd-0631-430b-8d83-4f43f3fc269f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Close', 'High', 'Low', 'Open', 'Volume', 'Movement', 'date'], dtype='object', name='Price')\n"
     ]
    }
   ],
   "source": [
    "# Flatten the MultiIndex columns\n",
    "stock_data.columns = stock_data.columns.get_level_values(0)\n",
    "\n",
    "# Verify the updated columns\n",
    "print(stock_data.columns)\n",
    "# Output: Index(['date', 'Movement'], dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c329a13-6bed-479d-8a7b-ce8944d9294a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Close', 'High', 'Low', 'Open', 'Volume', 'Movement', 'date'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Remove the column index name\n",
    "stock_data.columns.name = None\n",
    "\n",
    "# Verify the updated column names\n",
    "print(stock_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8b23909-dc37-45f2-8f3c-7b4ca718d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns: Ensure only the necessary colums are used\n",
    "stock_data = stock_data[['date', 'Movement']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b81a5ddb-3bf1-4f3a-91e5-7ea245bc4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' columns to datetime.date\n",
    "tweet_data['date'] = pd.to_datetime(tweet_data['date']).dt.date\n",
    "stock_data['date'] = pd.to_datetime(stock_data['date']).dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a73db3e-7dba-465f-b53d-a4aa0d375dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        index           timestamp                     text        date  \\\n",
      "0  1970-01-01 2024-10-10 14:30:00    Stock market booming!  2024-10-10   \n",
      "1  1970-01-01 2024-10-11 10:00:00  Tesla stock looks weak.  2024-10-11   \n",
      "\n",
      "   Movement  \n",
      "0         0  \n",
      "1         0  \n"
     ]
    }
   ],
   "source": [
    "# Perform the merge\n",
    "merged_data = pd.merge(tweet_data, stock_data, on='date', how='inner')\n",
    "\n",
    "# Print the merged DataFrame\n",
    "print(merged_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0361dd20-2a92-4d9d-ae2d-d2826466e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= merged_data['text']\n",
    "y= merged_data['Movement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c85cf6d0-6328-40a8-b516-ac05adbc3a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 0 0]\n",
      " [0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Convert text into numerical data using Bag of Words\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X_vectorized = vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "print(X_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4ec403b-9adf-44da-b6b0-56a094aabd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "17ddadd8-297a-4044-981a-0ef283d24d39",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Predict on the test set\u001b[39;00m\n\u001b[1;32m      9\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1301\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1299\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 1301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1305\u001b[0m     )\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1308\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(0)"
     ]
    }
   ],
   "source": [
    "#Training the ML model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316714fb-6a62-4a03-ae76-d472806dd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
